---
title: "Continuous SEM, with frequentist random intercept modeling with heteroscedasticity"
author: "Marieke Timmerman"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r load-libraries, echo = FALSE, warning = FALSE, message = FALSE}
#Load required R packages.
library(MASS)
library(tidyverse)
library(lme4)
library(nlme)
library(knitr)
source("R/helper_functions.R")
```


# Population data, SEM and cSEM per sumscore

We generate 'population' data according to a 2PLM model for nJ = 10 items, of size N = 100000. Then we approximate the SEM, and cSEM per sumscore. We do this for 10 replicates, and print the results in tables.
The item parameters are taken fixed across replicates, and the ability parameters vary across replicates.

```{r simulate-pop, echo = FALSE, warning = FALSE, message = FALSE}

nJ <- 10 #item
J <-10 #replicates
appr_pop_SEM <- data.frame(matrix(nrow = 1, ncol = J))
colnames(appr_pop_SEM) <- paste0("repl", 1:J) 

appr_pop_cSEM <- data.frame(matrix(nrow = nJ+1, ncol = J))
colnames(appr_pop_cSEM) <- paste0("repl", 1:J) 
rownames(appr_pop_cSEM) <- paste0("sumscore", 0:nJ) 

# Approximate population values using a very large simulated sample (assuming standard normal theta)
for (jj in 1:J ) {
X <- simulateGRM(100000, nJ = nJ, itemfixed = TRUE)
appr_pop_SEM[1,jj] <- sqrt(mean((rowSums(X$isc) - rowSums(X$true))^2))

Xplus <- rowSums(X$isc)
  for (ii in 0:nJ){
    sel <- Xplus == ii
    appr_pop_cSEM[ii + 1, jj] <- sqrt(mean((ii - rowSums(X$true)[sel])^2))
  }
}

kable(appr_pop_SEM, caption = "Approximate population SEM, for 10 replicates")
kable(appr_pop_cSEM, caption = "Approximate population cSEMs, for 10 replicates")
  
plot(0:10, appr_pop_cSEM[, 1], type="b", pch = 16, main = "cSEM as a function of sum scores, for replicate 1",
  xlab = "sumscore", ylab = "approximate cSEM")

```

Even with this large sample size there is some variability across replicates. Increasing the sample size does help in reducing this variability, so I am confident that this code is correct.

@Wilco: Wat ik noem: "Approximate population cSEMs" is dat niet. ZOu jij dit willen corrigeren, en aanvullen met de 3 andere opties erbij?



# Sample data, SEMs via alpha, lmer and lme (both with compound symmetry and without) 

Now, we generate sample data of size n = 1000. We estimate the SEM via coefficient alpha, and the estimated residuals of a rm-anova, estimated in 3 ways: with 2 types of model specifications (random intercept model without compound symmetry (cs), and with cs), and 2 functions (lmer, lme). We do this for 10 replicates, to assess the stability of estimation.

```{r simulate-sample_SEM, echo = FALSE, warning = FALSE, message = FALSE}
nJ <- 10 #item
J <-10 #replicates

sam_estSEMs <- data.frame(matrix(nrow = 4, ncol = J))
colnames(sam_estSEMs) <- paste0("repl", 1:J) 
rownames(sam_estSEMs) <- c("SEM_alpha","SEM_lmer","SEM_lme_nocs","SEM_lme_cs" )

for (jj in 1:J ) {
X <- simulateGRM(nN = 1000, nJ = nJ)$isc
Xwide <- as.data.frame(X)
colnames(Xwide) <- paste0("I", 1:ncol(X))
Xwide$Person <- rownames(X) %||% 1:nrow(X)
  
# Compute sample statistics
alpha <- psychometric::alpha(X)
SEM_alpha <- sqrt(sum(var(X)) * (1-alpha))

# Estimating the unconditional SEM for IRT generated data using different set-ups
  Xlong <- pivot_longer(
    Xwide,
    cols = -Person,
    names_to = "Item",
    values_to = "Score"
  )

  
# via lmer (from lme4) - random intercept model, persons random, items fixed
model <-  try(lmer(Score ~ Item + (1 | Person), data = Xlong, REML = TRUE)
    , silent = TRUE)
 if (inherits(model, "try-error")) {
   SEM_lmer  <- NA
  } else {
res_term <- as.data.frame(VarCorr(model))[2, 4]
SEM_lmer <- sqrt(res_term * nJ)
  }

# via nlme - no compound symmtry imposed
model <-  try(lme(fixed = Score ~ Item, random = ~1 | Person, data = Xlong)
    , silent = TRUE)
  
 if (inherits(model, "try-error")) {
   SEM_lme_nocs  <- NA
  } else {
  res_term <- as.numeric(VarCorr(model)[2, 1])
  SEM_lme_nocs <- sqrt(res_term * nJ)
  }

# via nlme compound symmetry imposed
model <-  try( lme
    (fixed = Score ~ Item, random = ~1 | Person, correlation = corCompSymm(form = ~1 | Person),
    data = Xlong)
    , silent = TRUE)
  
 if (inherits(model, "try-error")) {
   SEM_lme_cs  <- NA
  } else {
  res_term <- as.numeric(VarCorr(model)[2, 1])
  SEM_lme_cs <- sqrt(res_term * nJ)
  }
sam_estSEMs[,jj] <- c(SEM_alpha, SEM_lmer, SEM_lme_nocs, SEM_lme_cs)
}

kable(sam_estSEMs, caption = "Estimated SEMs, via alpha, lmer, lme without compound symmetry, and with compound symmetry, for 10 replicates")

```

The estimated SEMs are consistently equal across the four estimates (via alpha, lmer and lme). A NA is due to non-convergence problems.

In general, when trying out different replicates and settings, lmer showed less non-convergence issues than lme. Further, lme_cs (so with compound symmtry imposed) and lme_nocs (without compound symmtry imposed) specify the same models. The model specified with lme_nocs (and lmer) is simply the random intercept model, with level 2 persons, level 1 items, and the items as fixed effects. The model-implied covariance structure is the same as that in a compound symmetry model. Imposing compound symmetry explicitly may steer the algorithm in the correct direction - but in principle it is superfluous.



# Sample data, cSEMs per sumscore, via lmer and lme (with and without compound symmetry)
Now, we generate sample data of size n = 1000. We estimate the cSEMs per sum score, via the estimated residuals of a rm-anova, estimated in 3 ways: with 2 types of model specifications (random intercept model without compound symmetry (cs), and with cs), and 2 functions (lmer, lme). 

```{r simulate-sample_cSEM, echo = FALSE, warning = FALSE, message = FALSE}
nJ <- 10 #items

for (jj in 1:3) {  
sam_estcSEMs <- data.frame(matrix(nrow = 3, ncol = nJ+1))
colnames(sam_estcSEMs) <- paste0("sum", 0:nJ) 
rownames(sam_estcSEMs) <- c("cSEM_lmer","cSEM_lme_nocs","cSEM_lme_cs" )


X <- simulateGRM(nN = 1000, nJ = nJ)$isc
Xwide <- as.data.frame(X)
colnames(Xwide) <- paste0("I", 1:ncol(X))
Xwide$Person <- rownames(X) %||% 1:nrow(X)
  

Xplus <- rowSums(Xwide[, 1:nJ])
Xwide  <- Xwide %>% mutate(Xplus = Xplus)
 # head(Xwide)
  
  Xlong <- pivot_longer(
    Xwide,
    cols = 1:nJ,
    names_to = "Item",
    values_to = "Score"
  )
#  head(Xlong)
  
for (jjn in 0:nJ ) {  
Xlong_select  <- Xlong %>% filter(Xplus == jjn)

# via lmer (from lme4) - random intercept model, persons random, items fixed
model <-  try(lmer(Score ~ Item + (1 | Person), data = Xlong_select, REML = TRUE)
    , silent = TRUE)
 if (inherits(model, "try-error")) {
   cSEM_lmer  <- NA
  } else {
res_term <- as.data.frame(VarCorr(model))[2, 4]
cSEM_lmer <- sqrt(res_term * nJ)
}

# via nlme - no compound symmtry imposed
model <-  try(lme(fixed = Score ~ Item, random = ~1 | Person, data = Xlong_select)
    , silent = TRUE)
 if (inherits(model, "try-error")) {
   cSEM_lme_nocs  <- NA
  } else {
  res_term <- as.numeric(VarCorr(model)[2, 1])
  cSEM_lme_nocs <- sqrt(res_term * nJ)
  }

  
# via nlme compound symmetry imposed
model <-  try(lme
    (fixed = Score ~ Item, random = ~1 | Person, correlation = corCompSymm(form = ~1 | Person),
    data = Xlong_select)
    , silent = TRUE)
  
 if (inherits(model, "try-error")) {
   cSEM_lme_cs  <- NA
  } else {
  res_term <- as.numeric(VarCorr(model)[2, 1])
  cSEM_lme_cs <- sqrt(res_term * nJ)
  }
sam_estcSEMs[,jjn+1] <- c(cSEM_lmer, cSEM_lme_nocs, cSEM_lme_cs)
}

print(kable(sam_estcSEMs, caption = "Estimated cSEMs per sumscore, via lmer, lme without compound symmetry, and with compound symmetry, for one replicate",digits = 3))

}
```

Now, the cSEM_lme_cs estimates are consistently lower than the cSEMs as estimated with lmer and lme, without the cs.
Apparently, now requiring the superfluous parameter to estimate makes things differently. I think the cs model is overparametrized, and now results in a non-unique solution. 
It seems safer to continue with the specification without compound symmetry imposed.

# Sample data, cSEMs per sumscore, via modeling residual variances as a function of the sumscores

Now, we try to model the full sample data (N=1000) with a random effect model, allowing for heteroscedasticity, depending on the sum score (Xplus). We can only do via lme, since lmer does not facilitate heteroscedasticity.  
We do so via an exponential term. I fitted three models: 1. linear effect of Xplus; 2. quadratic effect of Xplus; 3. linear plus quadratic effect of Xplus.
To get an idea of the variability across replicates, I simulated 3 replicates, and provide the estimates in 3 successive tables.


```{r simulate-sample_cSEM_1model, echo = FALSE, warning = FALSE, message = FALSE, fig.show='hold'}
nJ <- 10 #items

for (jj in 1:3) {  
  
sam_estcSEMs_1M <- data.frame(matrix(nrow = 3, ncol = nJ+1))
#colnames(sam_estcSEMs_1M) <- paste0("sum", 0:nJ) 
#rownames(sam_estcSEMs_1M) <- c("cSEM_lme_lin","cSEM_lme_qua","cSEM_lme_lin+qua" )


X <- simulateGRM(nN = 1000, nJ = nJ)$isc
Xwide <- as.data.frame(X)
colnames(Xwide) <- paste0("I", 1:ncol(X))
Xwide$Person <- rownames(X) %||% 1:nrow(X)
Xsum   <- rowSums(Xwide[, 1:nJ])
Xc     <- Xsum - 5
Xplus  <- Xsum + 0.0001
Xplus2 <- Xc^2

X_eval <- seq(0, 10, by = 1)
Xplus_eval  <- X_eval + 0.0001
Xplus2_eval <- (X_eval - 5)^2

Xwide  <- Xwide %>% mutate(Xplus = Xplus, Xplus2)

Xlong <- pivot_longer(
Xwide,
    cols = 1:nJ,
    names_to = "Item",
    values_to = "Score"
  )

# via nlme - no compound symmetry imposed

#model 1: varExp, linear
model <-  try(lme(fixed = Score ~ Item, random = ~1 | Person, weights = varExp(form = ~ Xplus), data = Xlong)
    , silent = TRUE)
 if (inherits(model, "try-error")) {
   cSEM_1  <- matrix(nrow = 1, ncol = nJ+1, NA)
  } else {
cSEM_1 <- eval_resid_var(model, Xplus_eval) 
  }

plot_resid_var(model, Xlong$Xplus)

p <- plot_resid_var(model, Xlong$Xplus)
print(p)

#model 2: varExp, quadratic
model <- try(
  lme(
    fixed = Score ~ Item,
    random = ~1 | Person,
    weights = varExp(form = ~ Xplus2),
    data = Xlong
  ),
  silent = TRUE
)

if (inherits(model, "try-error")) {
  cSEM_2 <- matrix(NA, nrow = 1, ncol = nJ + 1)
} else {
  cSEM_2 <- eval_resid_var(model, Xplus2_eval)
  plot_resid_var(model, Xlong$Xplus)
}





#model 3: varExp, linear plus quadratic
model <-  try(lme(fixed = Score ~ Item, random = ~1 | Person, weights = varComb(
    varExp(form = ~ Xplus),
    varExp(form = ~ Xplus2)
  ), data = Xlong)
    , silent = TRUE)
 if (inherits(model, "try-error")) {
   cSEM_3  <- matrix(nrow = 1, ncol = nJ+1, NA)
  } else {
cSEM_3 <- eval_resid_var(model, Xplus_eval, Xplus2_eval) 
}


colnames(sam_estcSEMs_1M) <- paste0("sum", 0:nJ) 
rownames(sam_estcSEMs_1M) <- c("cSEM_lme_lin","cSEM_lme_qua","cSEM_lme_lin+qua" )


print(kable(sam_estcSEMs_1M, caption = "Estimated cSEMs per sumscore, via model-implied residual variances, for one replicate",digits = 3))
}
```


# Future work

** Willen we nog flexibelere variantiefuncties (splines, smooths), dan lijkt het erop dat we Bayesiaans moeten gaan, via brms, of Stan. Maar tot zover maar eerst even kijken of we dit uberhaupt een goede aanpak vinden.


